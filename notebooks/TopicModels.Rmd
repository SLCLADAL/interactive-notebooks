---
title: "Introduction to Topic Modeling with R"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

You can execute a chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

## Setting up

We will use several R packages in this section of the workshop. The pre-workshop instructions should have guided you through the process of installing the packages - if not you will need to run the following code:

```{r install, eval = F}
install.packages("tm")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
```

Then we need to activate the packages - you need to do this whether you installed the pacakges earlier, or did it now:

```{r}
library(tm)
library(topicmodels)
library(ggplot2)
library(reshape2)
```

## Topic models

Topic modelling is an unsupervised Machine Learning method which was developed for text mining. The method seeks to answer the question: given a collection of documents, can we identify what they are ‘about’?

Topic model algorithms look for patterns of co-occurences of words in documents. We assume that, if a document is about a certain topic, one would expect words that are related to that topic to appear in the document more often than in documents that deal with other topics. For instance, *dog* and *bone* will appear more often in documents about dogs whereas *cat* and *meow* will appear in documents about cats. Terms like *the* and *is* will, however, appear approximately equally in both.

A complete topic model consists of two sets of frequency distributions:

- for each topic, the probabilities of each word in the documents being part of that topic
- for each document, the probability of each topic being found in that document

In order for an algorithm to achieve this, we have to specify in advance how many topics we want to model. We want topics (represented by the words most strongly associated with them) to have some kind of semantic coherence. If we have too few topics, they will tend to be over-general; on the other hand,if we have too many topics, they may overlap or be impossible to interpret. There are methods which can help us estimate an optimal number of topics (see the [LADAL Tutorial](https://slcladal.github.io/topicmodels.html)); today we will only experiment with different numbers of topic and see the different output which results.

## Topic models and tweets

Topic models are based on patterns of co-occurrence of words. But tweets are very short and can provide only limited information of that kind. Therefore treating individual tweets as documents in a topic model analysis is not likely to work well. We need to combine tweets in some way to make larger documents. Today, we will split the Election 2019 data into four groups on the basis of the party affiliation of the candidates: Coalition, Greens, Labor and Others. This is not a workshop about data wrangling, so we have grouped the tweets into four separate files for you. 


```{r}
my_files <- list.files(pattern = "\\.csv$")
# inspect
my_files
```


This code chunk reads the four files (it assumes that they are all in the same directory and that there are no other .csv files in that directory). Then we loop through the files and iteratively combine the text of each group of tweets into a single vector of text. The lines which use the **gsub** function remove web links, weird characters, and the character entity for ampersand (\&amp;), a relic of which was showing up in topics in our initial exploration of this data.

```{r}
oztwit <- lapply(my_files, read.csv)
# extract tweet text and combine as single item
oztweets <- lapply(oztwit, function(x){
  x <- paste0(x$text, collapse = " ")
  x <- tolower(x)
  x <- gsub("http.*? ", " ", x)
    # remove weird characters
  x <- gsub("&amp;", "", x)
  x <- gsub("[^[:alnum:] \\#\\@]", "", x)
})
# inspect
str(oztweets)
```


## Cleaning the data

The **tm** (text mining) package provides useful tools for preprocessing text, so our next step is to make a Corpus object in tm using our four documents. We then apply various cleaning procedures, two of which need comment.

First, we remove **stop words** from the corpus. These are words which will add very little (if any) information to our analysis, but which are in many cases very common (words like *and* and *you*). Removing these words reduces the amount of data which has to be analysed and this speeds up the topic modeling procedure. Secondly, we apply a **stemming** procedure. Topic modeling is a semantic analysis. Therefore, we want different forms of a base word to be treated as the same. For example, *dog* and *dogs* will make the same semantic contribution to topics, and the same should be true for forms such as *eat*, *eats*, *eating* and *eaten*. One way of achieving this is to assign every word form to a base form (usually called a **lemma**), but this is time consuming and requires a very detailed dictionary. Instead, we exploit the fact that English inflectional morphology uses only suffixes (adding bits at the end of words). Stemming is the process of removing certain strings (e.g. *-ing*) from the end of words so that the kind of groups shown above all look the same. As you will see when you run the code, the output of this process can look odd at first!

```{r}
# create tm corpus
twitter_corpus <- Corpus(VectorSource(oztweets))

# check results
str(as.character(twitter_corpus[[4]]))

# preprocessing
processedCorpus <- tm_map(twitter_corpus, removeWords, stopwords("english"))
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)

# check again
str(as.character(processedCorpus[[4]]))
```

## The analysis

The topic modeling algorithm works on a **document-term matrix** - a table which tracks the number of occurrences of each term (word type) in each document. The line **dim(DTM)** shows us how many terms we have in our corpus (we already know how many documents there are).

We will start by looking for 20 topics - this is an edcuated guess! The algorithm uses random numbers for sampling and we need to set a seed for that process. Then we can run the analysis. We are using the most common algorithm for topic modeling, **Latent Dirichlet Allocation**, which is based on the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution).

```{r}
# compute document term matrix 
DTM <- DocumentTermMatrix(processedCorpus)
# have a look at the number of documents and terms in the matrix
dim(DTM)

# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
```

## Results

First, we can look at some of the mathematical properties of the model that has been constructed. We can see that it includes two sets of probability distributions. **beta** is the collection of distributions of terms over topics; this consists of a row for each topic, with each row containing the probabilities of terms occurring in that topic. Because each row is a probablity distribution, they all sum to 1. **theta** is the corresponding set of distributions of documents over topics.

The last line of code is where things start to get really interesting - this generates a listing for each topic of the ten terms most strongly associated with (most probable for) that topic. Do you think that any of these are interpretable?
```{r}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
nTerms(DTM)             
# topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms
rowSums(beta)            # rows in beta sum to 1
nDocs(DTM)               # size of collection
# for every document we have a probaility distribution of its contained topics
theta <- tmResult$topics 
dim(theta)               # nDocs(DTM) distributions over K topics
rowSums(theta)[1:4]     # rows in theta sum to 1
terms(topicModel, 10)
```

## Visualising our results

Now let's find out whether different topics are associated with different political groups. We can do this by creating a chart showing the level of association for each topic for each group. We give names to the topics by joining together the top five terms associated with each topic, we extract the probabilities from the theta of our model, create a data frame to hold this data, and then we use **ggplot** to make the graphic. What does it tell us?

```{r}
# give topics names
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")


exampleIds <- c(1,2,3,4)
N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")

# add party names to documents
parties <- gsub(".*/", "", my_files)
parties <- gsub(".csv", "", parties)

vizDataFrame$party <- ifelse(vizDataFrame$document == 1, parties[1],
                                ifelse(vizDataFrame$document == 2, parties[2],
                                       ifelse(vizDataFrame$document == 3, parties[3],
                                              ifelse(vizDataFrame$document == 4, parties[4], NA))))


# inspect
head(vizDataFrame)
```



```{r}
ggplot(data = vizDataFrame, aes(topic, value, fill = party), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ party, ncol = N) + 
  theme(legend.position = "none")
```


## Going further

You can go back and experiment with using different numbers of topics. If you look at the code under the heading **The analysis**, you will see the line:

K <- 20

You can change the number in that line and rerun all the following code to see what the results look like with either fewer topics or more topics.

