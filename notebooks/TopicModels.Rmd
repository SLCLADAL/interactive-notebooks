---
title: "Introduction to Topic Modeling with R"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

You can execute a chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

## Setting up

We will use several R packages in this section of the workshop. The pre-workshop instructions should have guided you through the process of installing the packages - if not you will need to run the following code:

```{r install, eval = F}
install.packages("tm")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
```

Then we need to activate the packages - you need to do this whether you installed the pacakges earlier, or did it now:

```{r}
library(tm)
library(topicmodels)
library(ggplot2)
library(reshape2)
```

## Topic models

Topic modelling is an unsupervised Machine Learning method which was developed for text mining. The method seeks to answer the question: given a collection of documents, can we identify what they are ‘about’?

Topic model algorithms look for patterns of co-occurences of words in documents. We assume that, if a document is about a certain topic, one would expect words that are related to that topic to appear in the document more often than in documents that deal with other topics. For instance, *dog* and *bone* will appear more often in documents about dogs whereas *cat* and *meow* will appear in documents about cats. Terms like *the* and *is* will, however, appear approximately equally in both.

A complete topic model consists of two sets of frequency distributions:

- for each topic, the probabilities of each word in the documents being part of that topic
- for each document, the probability of each topic being found in that document

In order for an algorithm to achieve this, we have to specify in advance how many topics we want to model. We want topics (represented by the words most strongly associated with them) to have some kind of semantic coherence. If we have too few topics, they will tend to be over-general; on the other hand,if we have too many topics, they may overlap or be impossible to interpret. There are methods which can help us estimate an optimal number of topics (see the [LADAL Tutorial](https://slcladal.github.io/topicmodels.html)); today we will only experiment with different numbers of topic and see the different output which results.

## Topic models and tweets

Topic models are based on patterns of co-occurrence of words. But tweets are very short and can provide only limited information of that kind. Therefore treating individual tweets as documents in a topic model analysis is not likely to work well. We need to combine tweets in some way to make larger documents. Today, we will use the Echidna and Platypus data we collected earlier.

```{r}
# Load a dataset of tweets, to find out what is going on with that particular corpus.
tweets = read.csv("platypus_minimal.csv")
# extract tweet text and combine as single item
normalised_tweets <- lapply(tweets$text, function(x){
  x <- tolower(x)
  x <- gsub("http.*? ", " ", x)
})
# inspect
str(normalised_tweets)
```


## Cleaning the data

The **tm** (text mining) package provides useful tools for preprocessing text, so our next step is to make a Corpus object in tm using our four documents. We then apply various cleaning procedures, two of which need comment.

First, we remove **stop words** and **whitespace** from the corpus.

```{r}
# create tm corpus
twitter_corpus <- Corpus(VectorSource(normalised_tweets))

# check results
str(as.character(twitter_corpus[[1]]))

# preprocessing
processedCorpus <- tm_map(twitter_corpus, removeWords, stopwords("english"))
processedCorpus <- tm_map(processedCorpus, stripWhitespace)

# check again
str(as.character(processedCorpus[[1]]))

processedCorpus
```

## The analysis

The topic modeling algorithm works on a **document-term matrix** - a table which tracks the number of occurrences of each term (word type) in each document. The line **dim(DTM)** shows us how many terms we have in our corpus (we already know how many documents there are).

We will start by looking for 3 topics - this is a guess! The algorithm uses random numbers for sampling and we need to set a seed for that process. Then we can run the analysis. We are using the most common algorithm for topic modeling, **Latent Dirichlet Allocation**, which is based on the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution).

```{r}
# compute document term matrix 
DTM <- DocumentTermMatrix(processedCorpus)
# have a look at the number of documents and terms in the matrix
dim(DTM)

# number of topics
K <- 3
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
```

## Results

First, we can look at some of the mathematical properties of the model that has been constructed. We can see that it includes two sets of probability distributions. **beta** is the collection of distributions of terms over topics; this consists of a row for each topic, with each row containing the probabilities of terms occurring in that topic. Because each row is a probablity distribution, they all sum to 1. **theta** is the corresponding set of distributions of documents over topics.

The last line of code is where things start to get really interesting - this generates a listing for each topic of the ten terms most strongly associated with (most probable for) that topic. Do you think that any of these are interpretable?
```{r}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
# lengthOfVocab
nTerms(DTM)             
# topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms
rowSums(beta)            # rows in beta sum to 1
nDocs(DTM)               # size of collection
# for every document we have a probaility distribution of its contained topics
theta <- tmResult$topics 
dim(theta)               # nDocs(DTM) distributions over K topics
rowSums(theta)[1:4]     # rows in theta sum to 1
terms(topicModel, 10)
```

## Visualising our results

Now let's find out whether different topics are associated with different political groups. We can do this by creating a chart showing the level of association for each topic for each group. We give names to the topics by joining together the top five terms associated with each topic, we extract the probabilities from the theta of our model, create a data frame to hold this data, and then we use **ggplot** to make the graphic. What does it tell us?

```{r}
# give topics names
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")

topicNames

N <- length(exampleIds)
# get topic proportions form example documents
topicProportionExamples <- theta

colnames(topicProportionExamples) <- topicNames

```


## Exercises:

1. How does changing the number of topic K impact your interpretation of the results?
2. What does running the same analysis for echidna_minimal.csv look like.

