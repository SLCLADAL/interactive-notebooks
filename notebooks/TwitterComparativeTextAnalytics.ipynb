{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWwHjS5UtEQ1"
   },
   "source": [
    "# Basic text analytics: counting words\n",
    "\n",
    "In this notebook, you will learn:\n",
    "1. how to prepare text data for analysis\n",
    "2. how to derive word frequency information for a corpus (a collection of texts)\n",
    "3. how to produce a visualisation comparing the frequency of occurrence of words in two corpora\n",
    "\n",
    "**Prerequisites**:\n",
    "1. basic familiarity with R\n",
    "3. The following R packages installed: **tm, NLP, stringi, magrittr**\n",
    "\n",
    "### What's in a word?\n",
    "\n",
    "The title above includes the phrase *counting words* - but what does *word* mean here? How would you define *word*?\n",
    "\n",
    "To get a sense of the problem, think about the following question:\n",
    "\n",
    "*How many words are there in the sentence '**The cat sat on the mat**'?*\n",
    "\n",
    "There are two reasonable answers to this question:\n",
    "- Answer 1: There are six words – six groups of characters separated in accordance with orthographic convention\n",
    "- Answer 2: There are five words – five unique groups of characters and one of those occurs twice\n",
    "Both answers are correct – but we are counting different things. The standard terms for these things are **tokens** and **types**.\n",
    "\n",
    "Types are the unique entities on our sample. If we count types, we are not interested in how many times each type occurs; after we have included a type in our count, we ignore repetitions. If we count tokens, we do care how often each type occurs; we count each repetition of each type\n",
    "\n",
    "We can also think about another question:\n",
    "\n",
    "*Are 'dog' and dogs' the same word?*\n",
    "\n",
    "They are distinct types – they are not identical aAnd they have to be distinct tokens (if they occur in the same text).\n",
    "But we still have an intuition that there is some abstract entity which includes both of them - for example, we would not expect a (monolingual) dictionary to have entries for both. The abstract entity goes by various names such as **lemma** and **listeme**. (We are not going to look at such entities today.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiRU-46mtjj1"
   },
   "source": [
    "### Getting set up\n",
    "We will be using four R packages in this exercise: **tm** which provides tools for text mining, **NLP** which tm requires, **stringi** which provides string processing tools, and **magrittr** which allows us to use pipe notation (more about this later). We do not want R to treat bits of text as factors, so we set the option which prevents that. We will be working with the data files which you created in the first part of the workshop and we assume that these are still locally available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109120,
     "status": "ok",
     "timestamp": 1637621573706,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "q37YidXltVsT",
    "outputId": "f426b90f-345b-43f1-c84d-74c8cca9f35b"
   },
   "outputs": [],
   "source": [
    "# Set up - Colab\n",
    "#install.packages('NLP')\n",
    "#install.packages('tm')\n",
    "#install.packages('stringi')\n",
    "#install.packages('magrittr')\n",
    "#library(\"NLP\")\n",
    "#library('tm')\n",
    "#library('stringi')\n",
    "#library('magrittr')\n",
    "\n",
    "# Set up - SWAN\n",
    "# create a local folder for the local files to live in\n",
    "# [Creates a folder call \"R\" in your CloudStor home]\n",
    "dir.create('/scratch/R', showWarnings = FALSE)\n",
    "\n",
    "# add the local folder to R's\n",
    ".libPaths(new='/scratch/R')\n",
    "\n",
    "# install packages locally\n",
    "install.packages(\"NLP\", lib=\"/scratch/R\")\n",
    "install.packages(\"tm\", lib=\"/scratch/R\")\n",
    "install.packages(\"stringi\", lib=\"/scratch/R\")\n",
    "install.packages(\"magrittr\", lib=\"/scratch/R\")\n",
    "# load package\n",
    "library(NLP, lib=\"/scratch/R\")\n",
    "library(tm, lib=\"/scratch/R\")\n",
    "library(stringi, lib=\"/scratch/R\")\n",
    "library(magrittr, lib=\"/scratch/R\") \n",
    "# set options\n",
    "options(stringsAsFactors = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWRngniJv8Hf"
   },
   "source": [
    "### Preparing the text\n",
    "First, we have to make our text available for analysis. There are several preprocessing operations which are more or less standard in text analysis, but first we need to think about how these operations relate to twitter data.\n",
    "\n",
    "The details of how text is tokenised vary from one tool to another. This means that in order to have *dog* and *dog,* treated as tokens of the same type, removing punctuation from text is sensible. This is normally done using a predefined list of punctuation characters such as this one:\n",
    "\n",
    "! \" # $ % & ' * + , . ) ( : ; < = > ? @ \\ / ] [ ^ ` { | } ~\n",
    "\n",
    "But this might be problematic for Twitter data! If we delete all punctuation from tweets, we will not be able to keep track of two important elements:\n",
    "- Hashtags e.g. #auspol\n",
    "- Handles e.g. @SimonMusgrave1\n",
    "\n",
    "One solution would be to change the relevant punctuation to text, but this is a bit clunky. Another possibility is to not delete those two punctuation marks. Text handling packages (like **tm** and **OpenNLP**) come with predefined punctuation lists and (in theory) we can edit those lists. But in practice it can be hard to track them down and it is easier to write a line of code to do a customised deletion.\n",
    "\n",
    "The next code cell does some basic cleaning operations on our data, such as getting rid of urls and new line characers (and a solution for the punctuation problem). We end by inspecting the data to make sure that we have the results we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1637622057643,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "oxkdkkYnzGNG"
   },
   "outputs": [],
   "source": [
    "# read data as csv file, read text as vector, create corpus\n",
    "# assumes data file is in the working directory\n",
    "echidna <- read.csv(\"echidna_minimal.csv\", header = TRUE, stringsAsFactors = FALSE, encoding = \"UTF-8\")\n",
    "# remove urls\n",
    "echidna$stripped_text <- gsub(\"http.*\",\"\",  echidna$text)\n",
    "echidna$stripped_text <- gsub(\"https.*\",\"\", echidna$stripped_text)\n",
    "# remove new lines\n",
    "echidna$stripped_text <- gsub(\"\\\\n\",\" \", echidna$stripped_text, fixed = TRUE)\n",
    "# punctuation \n",
    "echidna$stripped_text <- gsub(\"['.,!?\\\"*+%&;:<=>/)(^~`|)(}{]\", \"\", echidna$stripped_text)\n",
    "\n",
    "# inspect data\n",
    "str(echidna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qxRhKftzUVG"
   },
   "source": [
    "We also need to do something about emojis - they are part of the communicative resources of twitter and we don't want to just ignore them. The way we will do this is by replacing the emojis with their prose description. Notice that we have read in the data as UTF-8, i.e. Unicode encoding. We can search for the Unicode representations of emojis (which look like this: \\U+1F928) and then recover the description from the look-up table we download. Those descriptions have spaces in them and we delete those - our counting procedures look at single tokens, so *smiling face* will not be recorded as a unit but *smilingface* will. (Note that even if we decided we were not interested in emojis - and that is a reasonable decision in some cases - we would still need to be able to identify them in order to delete them.)\n",
    "\n",
    "The various search-and-replace operations in the previous and following code blocks use **regular expressions**. Regular expressions are a powerful way of specifying patterns of characters to be matched; if you would like to learn more about them, [this](http://solaris-8.tripod.com/regexp.pdf) is a good place to start.\n",
    "\n",
    "This code block also uses *piping*, mentioned above. The symbol **%>%** shows this - it can be read more or less as *and then*. So the first snippet which starts with *emoji* can be read as:\n",
    "Take the object **emoji**, extract all the elements in that object which match the specified pattern, *and then* take the output of that operation and replace the specified pattern with nothing *and then* take the output of that operation and trim any space from the strings *and then* assign the output to the object **emoji.chars**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5287,
     "status": "ok",
     "timestamp": 1637622130726,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "AMhkT2AyzuYA"
   },
   "outputs": [],
   "source": [
    "# encoded emojis to prose descriptions\n",
    "# read emoji info and get rid of documentation lines\n",
    "readLines(\"https://unicode.org/Public/emoji/5.0/emoji-test.txt\",\n",
    "          encoding=\"UTF-8\") %>%\n",
    "  stri_subset_regex(pattern = \"^[^#]\") %>%\n",
    "  stri_subset_regex(pattern = \".+\") -> emoji\n",
    "\n",
    "# get the emoji characters and clean them up\n",
    "emoji %>%\n",
    "  stri_extract_all_regex(pattern = \"# *.{1,2} *\") %>%\n",
    "  stri_replace_all_fixed(pattern = c(\"*\", \"#\"),\n",
    "                         replacement = \"\",\n",
    "                         vectorize_all=FALSE) %>%\n",
    "  stri_trim_both() -> emoji.chars\n",
    "\n",
    "# get the emoji character descriptions\n",
    "emoji %>%\n",
    "  stri_extract_all_regex(pattern = \"#.*$\") %>%\n",
    "  stri_replace_all_regex(pattern = \"# *.{1,2} *\",\n",
    "                         replacement = \"\") %>%\n",
    "  stri_trim_both() -> emoji.descriptions\n",
    "# collapse the descriptions\n",
    "emoji.descriptions <- gsub(\" \", '', emoji.descriptions, fixed = TRUE)\n",
    "\n",
    "# Replace Unicode characters with prose descriptions\n",
    "echidna$stripped_text <- stri_replace_all_regex(echidna$stripped_text, pattern = emoji.chars, replacement = paste(emoji.descriptions,\" \"), vectorize_all=FALSE)\n",
    "\n",
    "# inspect data\n",
    "head(echidna$stripped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdMp19Uxzu-p"
   },
   "source": [
    "### More preparation\n",
    "Now we can do the kind of preparation which we might do for any text data. Creating a Corpus object via **tm** is a good way to do this; it means we can  use **tm** tools to do some pre-processing and we will use other **tm** tools to do the counting later.\n",
    "\n",
    "Initially, we will apply four kinds of cleaning to our data:\n",
    "1. Making all characters lower case; otherwise *dog* and *Dog* will be treated as distinct types.\n",
    "2. Removing numbers; probably not important here, but e.g. page numbers can cause artefacts to appear in analyses\n",
    "3. Removing whitespace other than single spaces between tokens.\n",
    "\n",
    "(You will see some warning messages as this code runs - they are not important.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1637622161392,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "aZAmNFNrBb-v",
    "outputId": "8c9a09c9-974e-47ab-914d-ab477675fde2"
   },
   "outputs": [],
   "source": [
    "# make corpus\n",
    "echidna_corpus <- Corpus(VectorSource(echidna$stripped_text))\n",
    "\n",
    "# all lower case, remove whitespace, numbers\n",
    "echidna_corpus <- echidna_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removePunctuation) %>% tm_map(removeNumbers) %>% tm_map(stripWhitespace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIqA-oa-Bswi"
   },
   "source": [
    "### Doing some counting\n",
    "Now we can get the most basic numbers for our corpus: how many tokens and how many types do we have? We will do this by creating a matrix where each row represents a document and each column represents a type. The number of columns is the number of types, and we can get the number of tokens either as the sum of the sums of each row or as the sum of the sums of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1637622169754,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "bBdKHwuKBuZX",
    "outputId": "e4a726ca-9a1b-4bcf-a17d-76f30187857c"
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "dtm <- DocumentTermMatrix(echidna_corpus)\n",
    "# get word frequency list\n",
    "# token count \n",
    "tokens1 <- sum(rowSums(as.matrix(dtm)))\n",
    "print(paste0(\"Tokens: \", tokens1))\n",
    "# collapse matrix by summing over columns\n",
    "freq <- colSums(as.matrix(dtm))\n",
    "# length should be total number of terms - type count\n",
    "print(paste0('Types: ',length(freq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSKwSCPGCExA"
   },
   "source": [
    "### Looking at frequency\n",
    "Now we can look at what are the most frequently occurring types in our corpus. We take the column sums, the number of tokens for each type, and put them in a new matrix. Then we sort the matrix in descending order and view the 20 most frequent types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1637622176642,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "SDhyqtoYCFxI",
    "outputId": "b384d09a-f201-4a3b-ff35-5a234b7cf580"
   },
   "outputs": [],
   "source": [
    "# create sort order (descending)\n",
    "ord <- order(freq,decreasing=TRUE)\n",
    "# List top 20 terms in decreasing order of freq\n",
    "freq[ord][0:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bffUKS0CWDL"
   },
   "source": [
    "### Function words and content words\n",
    "These results are what we would expect to see for most samples of English text: function words like *the* and *and* are very common. The 20 most frequent words in the British National Corpus are *the, of, and, a, in, to, it, is, was, to, i, for, you, he, be, with, on, that, by, at*, all function words. \n",
    "\n",
    "Are there differences between the BNC list and the most frequent words in our data? Can we make any sense of these differences?\n",
    "\n",
    "In text analysis, it is very common to use a **stop list**, that is a list of words which will removed from the corpus before any analysis is performed. We will apply a stop list to our corpus and see what difference this makes to the list of most frequent words. (You can ignore the warning message again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1637622203119,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "VwIeWp-gCoHM",
    "outputId": "31f1bbd2-dbca-4623-a505-caa6c1b090cb"
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "docs1_stop <- tm_map(echidna_corpus, removeWords, stopwords(\"english\"))\n",
    "dtm <- DocumentTermMatrix(docs1_stop)\n",
    "freq <- colSums(as.matrix(dtm))\n",
    "ord <- order(freq,decreasing=TRUE)\n",
    "freq[ord][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m56dmBBhDIc1"
   },
   "source": [
    "### Comparing frequency\n",
    "We want to compare frequencies across two corpora, but we cannot do this by just comparing the counts directly because the corpora may differ in size. We expect that there will be more tokens of a given type in a larger corpus than in a smaller corpus just because of the difference in size, but we are interested in the relative frequency of types in the two corpora. To make this comparison possible, we need to **normalise** our frequency counts to some fixed number of tokens. Here, we will normalise frequency per 100k tokens. We also save the data for the 20 most common types as a data frame - we will use this when we make a visual comparison of the two text collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1637622232240,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "88UTAc9iDUYD",
    "outputId": "9cad20da-e134-4236-950e-e18170929ee9"
   },
   "outputs": [],
   "source": [
    "# make freq a data frame, add normalised freq\n",
    "types <- names(freq)\n",
    "freq_df <- data.frame(types, freq)\n",
    "ord_freq_C1 <- freq_df[order(freq, decreasing = TRUE),]\n",
    "ord_freq_C1['norm_freq'] <- ord_freq_C1$freq/(tokens1/100000)\n",
    "ord_freq_C1[0:20,]\n",
    "# store top 20 words for future use\n",
    "top20_echidna <- ord_freq_C1[0:20,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtt-8q5RKPEv"
   },
   "source": [
    "### Getting data from the second corpus\n",
    "\n",
    "Now, we will repeat our procedures for the second corpus - we won't bother with showing the frequent types without the stop list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 946
    },
    "executionInfo": {
     "elapsed": 11016,
     "status": "ok",
     "timestamp": 1637622286158,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "TJ0hi_ROKVoo",
    "outputId": "f18722fc-a275-4123-e199-fc6af107954c"
   },
   "outputs": [],
   "source": [
    "# corpus2\n",
    "# read data as csv file, read text as vector, create corpus\n",
    "platypus <- read.csv(\"platypus_minimal.csv\", header = TRUE, stringsAsFactors = FALSE, encoding = \"UTF-8\")\n",
    "# remove urls\n",
    "platypus$stripped_text <- gsub(\"http.*\",\"\",  platypus$text)\n",
    "platypus$stripped_text <- gsub(\"https.*\",\"\", platypus$stripped_text)\n",
    "# remove new lines\n",
    "platypus$stripped_text <- gsub(\"\\\\n\",\" \", platypus$stripped_text, fixed = TRUE)\n",
    "# change # and @ to text \n",
    "platypus$stripped_text <- gsub(\"#\",\"HASH\", platypus$stripped_text)\n",
    "platypus$stripped_text <- gsub(\"@\",\"AT\", platypus$stripped_text)\n",
    "\n",
    "platypus$stripped_text <- stri_replace_all_regex(platypus$stripped_text, pattern = emoji.chars, replacement = paste(emoji.descriptions,\" \"), vectorize_all=FALSE)\n",
    "\n",
    "# make corpus\n",
    "platypus_corpus <- Corpus(VectorSource(platypus$stripped_text))\n",
    "\n",
    "# all lower case,remove punctuation, numbers\n",
    "platypus_corpus <- platypus_corpus %>% tm_map(content_transformer(tolower)) %>% tm_map(removePunctuation) %>% tm_map(removeNumbers) %>% tm_map(stripWhitespace)\n",
    "\n",
    "# create document-term matrix\n",
    "dtm <- DocumentTermMatrix(platypus_corpus)\n",
    "#get word frequency list\n",
    "# token count \n",
    "tokens2 <- sum(rowSums(as.matrix(dtm)))\n",
    "print(paste0(\"Tokens: \", tokens2))\n",
    "# remove stopwords\n",
    "docs2_stop <- tm_map(platypus_corpus, removeWords, stopwords(\"english\"))\n",
    "# create document-term matrix\n",
    "dtm <- DocumentTermMatrix(docs2_stop)\n",
    "# collapse matrix by summing over columns\n",
    "freq <- colSums(as.matrix(dtm))\n",
    "# length should be total number of terms - type count\n",
    "print(paste0('Types: ',length(freq)))\n",
    "# create sort order (descending)\n",
    "ord <- order(freq,decreasing=TRUE)\n",
    "# make freq a data frame, add normalised freq\n",
    "types <- names(freq)\n",
    "freq_df <- data.frame(types, freq)\n",
    "ord_freq_C2 <- freq_df[order(freq, decreasing = TRUE),]\n",
    "ord_freq_C2['norm_freq'] <- ord_freq_C2$freq/(tokens2/100000)\n",
    "ord_freq_C2[0:20,]\n",
    "# inspect data\n",
    "top20_C2 <- ord_freq_C2[0:20,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGVFnrbPKdyL"
   },
   "source": [
    "### Making a visualisation\n",
    "The lists of 20 most frequent types overlap a little, but they are certainly not identical. We want to include all the words from both lists, so we combine them and then use the **unique()** function to get rid of duplicates. A side-by-side bar chart is a good way to visualise this data; the R base function **barplot()** will do this, but it needs the data as rows of a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1637622413876,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "D8KVxwFxKv6A",
    "outputId": "3f7fe6e0-009f-4233-8ef9-2e46d9a8775b"
   },
   "outputs": [],
   "source": [
    "# combine the two lists of common types\n",
    "types_combined <- top20_echidna['types']\n",
    "types_combined <- rbind(types_combined, top20_C2['types'])\n",
    "types_combined <- unique(types_combined)\n",
    "# make a matrix with the frequency data in two rows\n",
    "rows <- nrow(types_combined)\n",
    "bar_data <- matrix(1:rows*2, nrow = 2, ncol = rows)\n",
    "# get the normalised frequency values\n",
    "for (i in 1:rows) {\n",
    "    word <- types_combined[i,1]\n",
    "    freq_1 <- subset(ord_freq_C1, types == word)[3]\n",
    "    bar_data[1,i] <- freq_1[1,1]\n",
    "    freq_2 <- subset(ord_freq_C2, types == word)[3]\n",
    "    bar_data[2,i] <- freq_2[1,1]\n",
    "    }\n",
    "\n",
    "rows <- c('C1', 'C2')\n",
    "columns <- types_combined[,1]\n",
    "rownames(bar_data) <- rows\n",
    "colnames(bar_data) <- columns\n",
    "# inspect data\n",
    "bar_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyQCK6C9K4IY"
   },
   "source": [
    "We need to display the types vertically or we won't be able to see them all. Some of them are quite long words, so we reset the lower margin to make sure all of the characters will be visible. We also use the **las** parameter to make the text on each axis perpendicular to the axis. And then all that is left to do is to choose some colours that you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1637623279144,
     "user": {
      "displayName": "Simon Musgrave",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GguuB-J34iqcVjnFCISWHi0cw7BIxBP5Hgb2bMf=s64",
      "userId": "03928625455580756316"
     },
     "user_tz": -660
    },
    "id": "v9e3_DCKLFna",
    "outputId": "49e87c5b-6e73-421e-a7c5-f1c3e32641f6"
   },
   "outputs": [],
   "source": [
    "# set bottom margin to fit long words\n",
    "options(jupyter.plot_scale=1)\n",
    "par(mar=c(8,4,4,4))\n",
    "barplot(bar_data, beside = TRUE, las = 2, cex.names = 0.6,\n",
    "main = \"Comparative normalised frequencies (per100k)\", col = c(\"cyan\",\"grey\"))\n",
    "legend(\"topright\",\n",
    "c(\"Echidna\",\"Platypus\"),\n",
    "fill = c(\"cyan\",\"grey\"))\n",
    "axis(2, las=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outro\n",
    "\n",
    "Extract session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNO6PYY8bXdNimtOSzMRuD5",
   "collapsed_sections": [],
   "name": "AnalyseTwitterText.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R 4.1.1",
   "language": "R",
   "name": "ir4.1.1"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
